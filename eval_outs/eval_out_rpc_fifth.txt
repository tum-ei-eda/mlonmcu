======================
Host: return.eda.cit.tum.de (18C36T) RAM: 125.515 GiB
Scenario: EXECUTOR=rpc MULTIPLIER=64 ARGS=flow run aww --target etiss --backend tvmaot --platform mlif
Config: P=36 P_=1 B=1 N=4
> python3 -m mlonmcu.cli.main flow run aww --target etiss --backend tvmaot --platform mlif -c session.use_init_stage=0 -c print_report=0 -c runs_per_stage=0 --parallel 36 -c mlif.num_threads=4 -c tvmaot.num_threads=4 -c session.executor=rpc -c session.batch_size=1 --progress -c session.rpc_tracker=gpu2.eda.cit.tum.de:9000 -c session.rpc_key=default -c session.parallel_jobs=1 --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _ --config-gen _
INFO - Loading environment cache from file
INFO - Successfully initialized cache
Processing all runs:   0%|                                                           | 0/64 [00:00s]Processing all runs:   2%|▉                                                          | 1/64 [00:12s]Processing all runs:   3%|█▊                                                         | 2/64 [00:25s]Processing all runs:   5%|██▊                                                        | 3/64 [00:37s]Processing all runs:   6%|███▋                                                       | 4/64 [00:50s]Processing all runs:   8%|████▌                                                      | 5/64 [01:02s]Processing all runs:   9%|█████▌                                                     | 6/64 [01:14s]Processing all runs:  11%|██████▍                                                    | 7/64 [01:27s]Processing all runs:  12%|███████▍                                                   | 8/64 [01:39s]Processing all runs:  14%|████████▎                                                  | 9/64 [01:51s]Processing all runs:  16%|█████████                                                 | 10/64 [02:04s]Processing all runs:  17%|█████████▉                                                | 11/64 [02:16s]Processing all runs:  19%|██████████▉                                               | 12/64 [02:28s]Processing all runs:  20%|███████████▊                                              | 13/64 [02:41s]Processing all runs:  22%|████████████▋                                             | 14/64 [02:53s]Processing all runs:  23%|█████████████▌                                            | 15/64 [03:05s]Processing all runs:  25%|██████████████▌                                           | 16/64 [03:18s]Processing all runs:  27%|███████████████▍                                          | 17/64 [03:30s]Processing all runs:  28%|████████████████▎                                         | 18/64 [03:43s]Processing all runs:  30%|█████████████████▏                                        | 19/64 [03:55s]Processing all runs:  31%|██████████████████▏                                       | 20/64 [04:07s]Processing all runs:  33%|███████████████████                                       | 21/64 [04:20s]Processing all runs:  34%|███████████████████▉                                      | 22/64 [04:32s]Processing all runs:  36%|████████████████████▊                                     | 23/64 [04:44s]Processing all runs:  38%|█████████████████████▊                                    | 24/64 [04:57s]Processing all runs:  39%|██████████████████████▋                                   | 25/64 [05:09s]Processing all runs:  41%|███████████████████████▌                                  | 26/64 [05:21s]Processing all runs:  42%|████████████████████████▍                                 | 27/64 [05:34s]Processing all runs:  44%|█████████████████████████▍                                | 28/64 [05:46s]Processing all runs:  45%|██████████████████████████▎                               | 29/64 [05:58s]Processing all runs:  47%|███████████████████████████▏                              | 30/64 [06:11s]Processing all runs:  48%|████████████████████████████                              | 31/64 [06:23s]Processing all runs:  50%|█████████████████████████████                             | 32/64 [06:35s]Processing all runs:  52%|█████████████████████████████▉                            | 33/64 [06:48s]Processing all runs:  53%|██████████████████████████████▊                           | 34/64 [07:00s]Processing all runs:  55%|███████████████████████████████▋                          | 35/64 [07:12s]Processing all runs:  56%|████████████████████████████████▋                         | 36/64 [07:25s]ERROR - connection reset
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 332, in _process_rpc
    results = server.execute(run_initializers=run_initializers, until=until, parallel=parallel_jobs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 85, in execute
    response = base.recvjson(self._sock)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 123, in recvjson
    size = struct.unpack("<i", recvall(sock, 4))[0]
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 88, in recvall
    raise IOError("connection reset")
OSError: connection reset
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
OSError: connection reset
ERROR - An exception was thrown by a worker during simulation
Processing all runs:  58%|█████████████████████████████████▌                        | 37/64 [07:31s][2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:02]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
[2024-05-22 13:46:07]::WARNING - Cannot connect to tracker ('gpu2.eda.cit.tum.de', 9000), retry in 5 secs...
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
Processing all runs:  59%|██████████████████████████████████▍                       | 38/64 [09:25s]ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
ERROR - Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 193, in connect_with_retry
    sock.connect(addr)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 330, in _process_rpc
    server = tracker.request_server(key=rpc_config.key)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 226, in request_server
    self._connect()
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc.py", line 173, in _connect
    self._sock = base.connect_with_retry(self._addr, timeout=timeout)
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/rpc_utils.py", line 200, in connect_with_retry
    raise RuntimeError(f"Failed to connect to server {str(addr)}")
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/git/mlonmcu/mlonmcu/mlonmcu/session/schedule.py", line 520, in _join_futures
    batch_res = f.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Failed to connect to server ('gpu2.eda.cit.tum.de', 9000)
ERROR - An exception was thrown by a worker during simulation
Processing all runs: 100%|██████████████████████████████████████████████████████████| 64/64 [09:25s]WARNING - 36 out or 64 runs completed successfully!
INFO - Summary:
	None: 	28 failed run(s): 36 61 37 60 43 47 57 42 44 63 62 54 51 41 50 58 48 52 56 40 53 46 59 39 38 55 45 49
INFO - Postprocessing session report
ERROR - Session Postprocesses are not supported in pickable mode!
INFO - [session-59] Done processing runs
ERROR - At least one error occured!

Command exited with non-zero status 1
Elapsed:9:26.75, CPU: 1%
