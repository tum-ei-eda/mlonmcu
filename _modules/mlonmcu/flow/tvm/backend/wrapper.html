

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mlonmcu.flow.tvm.backend.wrapper &mdash; ML on MCU 0.7.dev0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/css/theme.css?v=e59714d7" />

  
      <script src="../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../_static/documentation_options.js?v=3bfbed7b"></script>
      <script src="../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../index.html" class="icon icon-home">
            ML on MCU
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../readme.html">ML on MCU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../design.html">Important Terms and Design Decisions (RFC)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../components.html">Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../components.html#components-in-detail">Components in Detail</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../environments.html">Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../postprocess.html">Postprocesses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../logging.html">Logging and Verbosity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../docs.html">Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../authors.html">Credits</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../history.html">History</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">ML on MCU</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../../index.html">Module code</a></li>
          <li class="breadcrumb-item"><a href="../../../flow.html">mlonmcu.flow</a></li>
      <li class="breadcrumb-item active">mlonmcu.flow.tvm.backend.wrapper</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for mlonmcu.flow.tvm.backend.wrapper</h1><div class="highlight"><pre>
<span></span><span class="c1">#</span>
<span class="c1"># Copyright (c) 2022 TUM Department of Electrical and Computer Engineering.</span>
<span class="c1">#</span>
<span class="c1"># This file is part of MLonMCU.</span>
<span class="c1"># See https://github.com/tum-ei-eda/mlonmcu.git for further info.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>
<span class="sd">&quot;&quot;&quot;TODO&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">ceil</span><span class="p">,</span> <span class="n">log2</span>

<span class="c1"># TODO: use this</span>
<span class="c1"># from tvm.relay.backend.utils import mangle_module_name</span>


<div class="viewcode-block" id="calc_pages">
<a class="viewcode-back" href="../../../../../mlonmcu.flow.tvm.backend.html#mlonmcu.flow.tvm.backend.wrapper.calc_pages">[docs]</a>
<span class="k">def</span> <span class="nf">calc_pages</span><span class="p">(</span><span class="n">workspace_size</span><span class="p">,</span> <span class="n">page_size</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">10</span><span class="p">):</span>
    <span class="c1"># Determine the number of required pages</span>
    <span class="k">assert</span> <span class="n">workspace_size</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Workspace size can not be negative&quot;</span>
    <span class="n">crtPageSizeLog2</span> <span class="o">=</span> <span class="n">log2</span><span class="p">(</span><span class="n">page_size</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">crtPageSizeLog2</span> <span class="o">==</span> <span class="nb">int</span><span class="p">(</span><span class="n">crtPageSizeLog2</span><span class="p">),</span> <span class="s2">&quot;Page size has to be a power of two&quot;</span>
    <span class="n">crtNumPages</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="n">workspace_size</span> <span class="o">/</span> <span class="n">page_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">crtNumPages</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">crtPageSizeLog2</span><span class="p">)</span></div>



<div class="viewcode-block" id="generate_wrapper_header">
<a class="viewcode-back" href="../../../../../mlonmcu.flow.tvm.backend.html#mlonmcu.flow.tvm.backend.wrapper.generate_wrapper_header">[docs]</a>
<span class="k">def</span> <span class="nf">generate_wrapper_header</span><span class="p">():</span>
    <span class="n">out</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;#ifndef TVM_WRAPPER_H</span>
<span class="s2">#define TVM_WRAPPER_H</span>

<span class="s2">#include &lt;stddef.h&gt;</span>

<span class="s2">int TVMWrap_Init();</span>
<span class="s2">void *TVMWrap_GetInputPtr(int index);</span>
<span class="s2">size_t TVMWrap_GetInputSize(int index);</span>
<span class="s2">size_t TVMWrap_GetNumInputs();</span>
<span class="s2">int TVMWrap_Run();</span>
<span class="s2">void *TVMWrap_GetOutputPtr(int index);</span>
<span class="s2">size_t TVMWrap_GetOutputSize(int index);</span>
<span class="s2">size_t TVMWrap_GetNumOutputs();</span>

<span class="s2">#endif  // TVM_WRAPPER_H</span>
<span class="s2">&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">out</span></div>



<div class="viewcode-block" id="generate_header">
<a class="viewcode-back" href="../../../../../mlonmcu.flow.tvm.backend.html#mlonmcu.flow.tvm.backend.wrapper.generate_header">[docs]</a>
<span class="k">def</span> <span class="nf">generate_header</span><span class="p">():</span>
    <span class="n">time</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
    <span class="n">header</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;// This file is generated. Do not edit.</span>
<span class="s2">// Generated on: </span><span class="si">{</span><span class="n">time</span><span class="si">}</span>
<span class="s2">&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">header</span></div>



<div class="viewcode-block" id="generate_common_includes">
<a class="viewcode-back" href="../../../../../mlonmcu.flow.tvm.backend.html#mlonmcu.flow.tvm.backend.wrapper.generate_common_includes">[docs]</a>
<span class="k">def</span> <span class="nf">generate_common_includes</span><span class="p">():</span>
    <span class="k">return</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">#include &lt;stdlib.h&gt;</span>
<span class="s2">#include &lt;stdarg.h&gt;</span>
<span class="s2">#include &lt;dlpack/dlpack.h&gt;</span>
<span class="s2">#include &quot;tvm/runtime/crt/error_codes.h&quot;</span>
<span class="s2">#include &quot;tvm/runtime/c_runtime_api.h&quot;</span>
<span class="s2">#include &quot;printing.h&quot;</span>
<span class="s2">#include &quot;exit.h&quot;</span>
<span class="s2">&quot;&quot;&quot;</span></div>



<div class="viewcode-block" id="generate_graph_includes">
<a class="viewcode-back" href="../../../../../mlonmcu.flow.tvm.backend.html#mlonmcu.flow.tvm.backend.wrapper.generate_graph_includes">[docs]</a>
<span class="k">def</span> <span class="nf">generate_graph_includes</span><span class="p">():</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">generate_common_includes</span><span class="p">()</span>
    <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">#include &quot;tvm/runtime/crt/packed_func.h&quot;</span>
<span class="s2">#include &quot;tvm/runtime/crt/crt.h&quot;</span>
<span class="s2">#include &quot;tvm/runtime/crt/graph_executor.h&quot;</span>
<span class="s2">#include &quot;tvm/runtime/crt/page_allocator.h&quot;</span>

<span class="s2">&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">out</span></div>



<div class="viewcode-block" id="generate_aot_includes">
<a class="viewcode-back" href="../../../../../mlonmcu.flow.tvm.backend.html#mlonmcu.flow.tvm.backend.wrapper.generate_aot_includes">[docs]</a>
<span class="k">def</span> <span class="nf">generate_aot_includes</span><span class="p">(</span><span class="n">allocator</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">generate_common_includes</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">allocator</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">+=</span> <span class="s1">&#39;#include &quot;tvm/runtime/crt/stack_allocator.h&quot;</span><span class="se">\n</span><span class="s1">&#39;</span>
    <span class="k">return</span> <span class="n">out</span></div>



<div class="viewcode-block" id="fill">
<a class="viewcode-back" href="../../../../../mlonmcu.flow.tvm.backend.html#mlonmcu.flow.tvm.backend.wrapper.fill">[docs]</a>
<span class="k">def</span> <span class="nf">fill</span><span class="p">(</span><span class="n">template</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">string</span><span class="o">.</span><span class="n">Template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span><span class="o">.</span><span class="n">substitute</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>



<div class="viewcode-block" id="getSizes">
<a class="viewcode-back" href="../../../../../mlonmcu.flow.tvm.backend.html#mlonmcu.flow.tvm.backend.wrapper.getSizes">[docs]</a>
<span class="k">def</span> <span class="nf">getSizes</span><span class="p">(</span><span class="n">tensors</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="s2">&quot;size_t sizes[] = { &quot;</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">+=</span> <span class="nb">str</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">size</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;, &quot;</span>
    <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;};&quot;</span>
    <span class="k">return</span> <span class="n">out</span></div>



<div class="viewcode-block" id="write_tvmrt_wrapper">
<a class="viewcode-back" href="../../../../../mlonmcu.flow.tvm.backend.html#mlonmcu.flow.tvm.backend.wrapper.write_tvmrt_wrapper">[docs]</a>
<span class="k">def</span> <span class="nf">write_tvmrt_wrapper</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">graph</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">model_info</span><span class="p">,</span> <span class="n">workspace_size</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">generate_tvmrt_wrapper</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">model_info</span><span class="p">,</span> <span class="n">workspace_size</span><span class="p">)</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">text</span><span class="p">)</span></div>



<div class="viewcode-block" id="generate_tvmrt_wrapper">
<a class="viewcode-back" href="../../../../../mlonmcu.flow.tvm.backend.html#mlonmcu.flow.tvm.backend.wrapper.generate_tvmrt_wrapper">[docs]</a>
<span class="k">def</span> <span class="nf">generate_tvmrt_wrapper</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">model_info</span><span class="p">,</span> <span class="n">workspace_size</span><span class="p">,</span> <span class="n">debug_arena</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">crtNumPages</span><span class="p">,</span> <span class="n">crtPageSizeLog2</span> <span class="o">=</span> <span class="n">calc_pages</span><span class="p">(</span><span class="n">workspace_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">escapeJson</span><span class="p">(</span><span class="n">j</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">j</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;&quot;&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\\</span><span class="s1">&quot;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\\\n</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">toCArray</span><span class="p">(</span><span class="nb">bin</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">bin</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">+=</span> <span class="nb">hex</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;, &quot;</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span> <span class="nf">getMeta</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">withNames</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="k">if</span> <span class="n">withNames</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="s2">&quot;const char *names[] = { &quot;</span>
            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">+=</span> <span class="s1">&#39;&quot;&#39;</span> <span class="o">+</span> <span class="n">t</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;&quot;, &#39;</span>
            <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;};</span><span class="se">\n</span><span class="s2">    &quot;</span>

        <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;DLDataType dtypes[] = {&quot;</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="s2">&quot;float32&quot;</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;{kDLFloat, 32, 1}&quot;</span>
            <span class="k">elif</span> <span class="n">t</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="s2">&quot;uint8&quot;</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;{kDLUInt, 8, 1}&quot;</span>
            <span class="k">elif</span> <span class="n">t</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="s2">&quot;int8&quot;</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;{kDLInt, 8, 1}&quot;</span>
            <span class="k">elif</span> <span class="n">t</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="s2">&quot;uint64&quot;</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;{kDLUInt, 64, 1}&quot;</span>
            <span class="k">elif</span> <span class="n">t</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="s2">&quot;int64&quot;</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;{kDLInt, 64, 1}&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid type: </span><span class="si">{</span><span class="n">t</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;, &quot;</span>
        <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;};</span><span class="se">\n</span><span class="s2">    &quot;</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tensors</span><span class="p">):</span>
            <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;int64_t shape_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;[] = { &quot;</span>
            <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">+=</span> <span class="nb">str</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;, &quot;</span>
            <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;};</span><span class="se">\n</span><span class="s2">    &quot;</span>
        <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;int64_t *shapes[] = { &quot;</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tensors</span><span class="p">):</span>
            <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;shape_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;, &quot;</span>
        <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;};</span><span class="se">\n</span><span class="s2">&quot;</span>

        <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;    size_t ndims[] = { &quot;</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tensors</span><span class="p">):</span>
            <span class="n">out</span> <span class="o">+=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;, &quot;</span>
        <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;};</span><span class="se">\n</span><span class="s2">    &quot;</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tensors</span><span class="p">):</span>
            <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;static uint8_t data_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;[&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">size</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;];</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;    uint8_t *data[] = { &quot;</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tensors</span><span class="p">):</span>
            <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;data_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;, &quot;</span>
        <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;};&quot;</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="n">out</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="n">out</span> <span class="o">+=</span> <span class="n">generate_header</span><span class="p">()</span>
    <span class="n">out</span> <span class="o">+=</span> <span class="n">generate_graph_includes</span><span class="p">()</span>
    <span class="n">out</span> <span class="o">+=</span> <span class="s1">&#39;const char * const g_graph = &quot;&#39;</span> <span class="o">+</span> <span class="n">escapeJson</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;&quot;;</span><span class="se">\n</span><span class="s1">&#39;</span>
    <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;const char g_params[] = { &quot;</span> <span class="o">+</span> <span class="n">toCArray</span><span class="p">(</span><span class="n">params</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">};</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;const uint64_t g_params_size = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;;</span><span class="se">\n</span><span class="s2">&quot;</span>

    <span class="n">mainCode</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>


<span class="s2">#define CRT_MEMORY_NUM_PAGES $</span><span class="si">{numPages}</span>
<span class="s2">#define CRT_MEMORY_PAGE_SIZE_LOG2 $</span><span class="si">{pageSizeLog2}</span>
<span class="s2">&quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">debug_arena</span><span class="p">:</span>  <span class="c1"># This will enable the feature only if it is not overwritten by the user/compiler</span>
        <span class="n">mainCode</span> <span class="o">+=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">#ifndef DEBUG_ARENA_USAGE</span>
<span class="s2">#define DEBUG_ARENA_USAGE 1</span>
<span class="s2">#endif</span>
<span class="s2">&quot;&quot;&quot;</span>

    <span class="n">mainCode</span> <span class="o">+=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">#ifdef DEBUG_ARENA_USAGE</span>
<span class="s2">size_t max_arena_usage = 0;</span>
<span class="s2">#endif</span>

<span class="s2">static uint8_t g_crt_memory[CRT_MEMORY_NUM_PAGES * (1 &lt;&lt; CRT_MEMORY_PAGE_SIZE_LOG2)];</span>
<span class="s2">static MemoryManagerInterface* g_memory_manager;</span>

<span class="s2">/*! </span><span class="se">\\</span><span class="s2">brief macro to do C API call */</span>
<span class="s2">#define TVM_CCALL(func)                                                                   </span><span class="se">\\</span>
<span class="s2">    do {                                                                                  </span><span class="se">\\</span>
<span class="s2">        tvm_crt_error_t ret = (func);                                                     </span><span class="se">\\</span>
<span class="s2">        if (ret != kTvmErrorNoError) {                                                    </span><span class="se">\\</span>
<span class="s2">            TVMLogf(&quot;</span><span class="si">%s</span><span class="s2">: </span><span class="si">%d</span><span class="s2">: error: </span><span class="si">%s</span><span class="se">\\</span><span class="s2">n&quot;, __FILE__, __LINE__, TVMGetLastError());       </span><span class="se">\\</span>
<span class="s2">            TVMPlatformAbort(ret);                                                        </span><span class="se">\\</span>
<span class="s2">        }                                                                                 </span><span class="se">\\</span>
<span class="s2">    } while (0)</span>

<span class="s2">TVMModuleHandle TVMArgs_AsModuleHandle(const TVMArgs* args, size_t index);</span>

<span class="s2">void __attribute__((noreturn)) TVMPlatformAbort(tvm_crt_error_t code)</span>
<span class="s2">{</span>
<span class="s2">    mlonmcu_exit(1);</span>
<span class="s2">}</span>

<span class="s2">void TVMLogf(const char* msg, ...)</span>
<span class="s2">{</span>
<span class="s2">    va_list args;</span>
<span class="s2">    va_start(args, msg);</span>
<span class="s2">    DBGPRINTF(msg, args);</span>
<span class="s2">    va_end(args);</span>
<span class="s2">}</span>

<span class="s2">tvm_crt_error_t TVMPlatformMemoryAllocate(size_t num_bytes, DLDevice dev, void** out_ptr)</span>
<span class="s2">{</span>
<span class="s2">    tvm_crt_error_t ret = g_memory_manager-&gt;Allocate(g_memory_manager, num_bytes, dev, out_ptr);</span>
<span class="s2">#ifdef DEBUG_ARENA_USAGE</span>
<span class="s2">    // Use this to estimate the required number of pages</span>
<span class="s2">    // Run in DEBUG mode in insert value of MAX printed last into the following equation:</span>
<span class="s2">    // (This will round to the next power of 2 which might not be wanted!)</span>
<span class="s2">    // num_pages = 2**ceil(log2(MAX/page_size))</span>
<span class="s2">    size_t end = (size_t)(*out_ptr-(void*)g_crt_memory)+num_bytes;</span>
<span class="s2">    if (end &gt; max_arena_usage)</span>
<span class="s2">    {</span>
<span class="s2">        max_arena_usage = end;</span>
<span class="s2">    }</span>
<span class="s2">#endif</span>
<span class="s2">    return ret;</span>
<span class="s2">}</span>

<span class="s2">tvm_crt_error_t TVMPlatformMemoryFree(void* ptr, DLDevice dev)</span>
<span class="s2">{</span>
<span class="s2">    return g_memory_manager-&gt;Free(g_memory_manager, ptr, dev);</span>
<span class="s2">}</span>

<span class="s2">tvm_crt_error_t TVMPlatformTimerStart()</span>
<span class="s2">{</span>
<span class="s2">    return kTvmErrorFunctionCallNotImplemented;</span>
<span class="s2">}</span>

<span class="s2">tvm_crt_error_t TVMPlatformTimerStop(double* elapsed_time_seconds)</span>
<span class="s2">{</span>
<span class="s2">  return kTvmErrorFunctionCallNotImplemented;</span>
<span class="s2">}</span>

<span class="s2">void *g_handle = NULL;</span>

<span class="s2">int TVMWrap_Init()</span>
<span class="s2">{</span>
<span class="s2">    int64_t device_type = kDLCPU;</span>
<span class="s2">    int64_t device_id = 0;</span>

<span class="s2">    TVMByteArray params;</span>
<span class="s2">    params.data = g_params;</span>
<span class="s2">    params.size = g_params_size;</span>

<span class="s2">    DLDevice dev;</span>
<span class="s2">    dev.device_type = (DLDeviceType)device_type;</span>
<span class="s2">    dev.device_id = device_id;</span>

<span class="s2">    // get pointers</span>
<span class="s2">    TVM_CCALL(PageMemoryManagerCreate(&amp;g_memory_manager, g_crt_memory, sizeof(g_crt_memory),</span>
<span class="s2">                                      CRT_MEMORY_PAGE_SIZE_LOG2));</span>
<span class="s2">    TVM_CCALL(TVMInitializeRuntime());</span>
<span class="s2">    TVMPackedFunc pf;</span>
<span class="s2">    TVMArgs args = TVMArgs_Create(NULL, NULL, 0);</span>
<span class="s2">    TVM_CCALL(TVMPackedFunc_InitGlobalFunc(&amp;pf, &quot;runtime.SystemLib&quot;, &amp;args));</span>
<span class="s2">    TVM_CCALL(TVMPackedFunc_Call(&amp;pf));</span>

<span class="s2">    TVMModuleHandle mod_syslib = TVMArgs_AsModuleHandle(&amp;pf.ret_value, 0);</span>

<span class="s2">    // run modules</span>
<span class="s2">    TVMGraphExecutor* graph_executor = NULL;</span>
<span class="s2">    TVM_CCALL(TVMGraphExecutor_Create(g_graph, mod_syslib, &amp;dev, &amp;graph_executor));</span>
<span class="s2">    TVM_CCALL(TVMGraphExecutor_LoadParams(graph_executor, params.data, params.size));</span>

<span class="s2">    //return graph_executor;</span>
<span class="s2">    g_handle = graph_executor;</span>
<span class="s2">    return 0;  // TODO</span>
<span class="s2">}</span>

<span class="s2">void *TVMWrap_GetInputPtr(int index)</span>
<span class="s2">{</span>
<span class="s2">    $</span><span class="si">{inMeta}</span>

<span class="s2">    DLTensor input;</span>
<span class="s2">    input.data = (void*)data[index];</span>
<span class="s2">    DLDevice device = {kDLCPU, 0};</span>
<span class="s2">    input.device = device;</span>
<span class="s2">    input.ndim = ndims[index];</span>
<span class="s2">    input.dtype = dtypes[index];</span>
<span class="s2">    input.shape = shapes[index];</span>
<span class="s2">    input.strides = NULL;</span>
<span class="s2">    input.byte_offset = 0;</span>

<span class="s2">    TVMGraphExecutor* graph_executor = (TVMGraphExecutor*)g_handle;</span>
<span class="s2">    TVMGraphExecutor_SetInput(graph_executor, names[index], &amp;input);</span>

<span class="s2">    return data[index];</span>
<span class="s2">}</span>

<span class="s2">size_t TVMWrap_GetInputSize(int index)</span>
<span class="s2">{</span>
<span class="s2">    $</span><span class="si">{inSizes}</span>

<span class="s2">    return sizes[index];</span>
<span class="s2">}</span>

<span class="s2">size_t TVMWrap_GetNumInputs()</span>
<span class="s2">{</span>
<span class="s2">    return $</span><span class="si">{numInputs}</span><span class="s2">;</span>
<span class="s2">}</span>

<span class="s2">int TVMWrap_Run()</span>
<span class="s2">{</span>
<span class="s2">    TVMGraphExecutor* graph_executor = (TVMGraphExecutor*)g_handle;</span>
<span class="s2">    TVMGraphExecutor_Run(graph_executor);</span>
<span class="s2">#if DEBUG_ARENA_USAGE</span>
<span class="s2">    DBGPRINTF(&quot;</span><span class="se">\\</span><span class="s2">nGraph executor arena max usage after model invocation: </span><span class="si">%lu</span><span class="s2"> bytes</span><span class="se">\\</span><span class="s2">n&quot;, max_arena_usage);</span>
<span class="s2">#endif  // DEBUG_ARENA_USAGE</span>
<span class="s2">    return 0;  // TODO</span>
<span class="s2">}</span>

<span class="s2">void *TVMWrap_GetOutputPtr(int index)</span>
<span class="s2">{</span>
<span class="s2">    $</span><span class="si">{outMeta}</span>

<span class="s2">    DLTensor output;</span>
<span class="s2">    output.data = (void*)data[index];</span>
<span class="s2">    DLDevice device = {kDLCPU, 0};</span>
<span class="s2">    output.device = device;</span>
<span class="s2">    output.ndim = ndims[index];</span>
<span class="s2">    output.dtype = dtypes[index];</span>
<span class="s2">    output.shape = shapes[index];</span>
<span class="s2">    output.strides = NULL;</span>
<span class="s2">    output.byte_offset = 0;</span>

<span class="s2">    TVMGraphExecutor* graph_executor = (TVMGraphExecutor*)g_handle;</span>
<span class="s2">    TVMGraphExecutor_GetOutput(graph_executor, index, &amp;output);</span>

<span class="s2">    return data[index];</span>
<span class="s2">}</span>

<span class="s2">size_t TVMWrap_GetOutputSize(int index)</span>
<span class="s2">{</span>
<span class="s2">    $</span><span class="si">{outSizes}</span>

<span class="s2">    return sizes[index];</span>
<span class="s2">}</span>

<span class="s2">size_t TVMWrap_GetNumOutputs()</span>
<span class="s2">{</span>
<span class="s2">    return $</span><span class="si">{numOutputs}</span><span class="s2">;</span>
<span class="s2">}</span>
<span class="s2">&quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">+=</span> <span class="n">fill</span><span class="p">(</span>
        <span class="n">mainCode</span><span class="p">,</span>
        <span class="n">inMeta</span><span class="o">=</span><span class="n">getMeta</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">in_tensors</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
        <span class="n">outMeta</span><span class="o">=</span><span class="n">getMeta</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">out_tensors</span><span class="p">),</span>
        <span class="n">inSizes</span><span class="o">=</span><span class="n">getSizes</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">in_tensors</span><span class="p">),</span>
        <span class="n">outSizes</span><span class="o">=</span><span class="n">getSizes</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">out_tensors</span><span class="p">),</span>
        <span class="n">numInputs</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">in_tensors</span><span class="p">),</span>
        <span class="n">numOutputs</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">out_tensors</span><span class="p">),</span>
        <span class="n">numPages</span><span class="o">=</span><span class="n">crtNumPages</span><span class="p">,</span>
        <span class="n">pageSizeLog2</span><span class="o">=</span><span class="n">crtPageSizeLog2</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>



<div class="viewcode-block" id="generate_tvmaot_wrapper">
<a class="viewcode-back" href="../../../../../mlonmcu.flow.tvm.backend.html#mlonmcu.flow.tvm.backend.wrapper.generate_tvmaot_wrapper">[docs]</a>
<span class="k">def</span> <span class="nf">generate_tvmaot_wrapper</span><span class="p">(</span><span class="n">model_info</span><span class="p">,</span> <span class="n">workspace_size</span><span class="p">,</span> <span class="n">mod_name</span><span class="p">,</span> <span class="n">api</span><span class="o">=</span><span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="n">debug_arena</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">modPrefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;tvmgen_</span><span class="si">{</span><span class="n">mod_name</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="k">def</span> <span class="nf">writeTensors</span><span class="p">(</span><span class="n">in_tensors</span><span class="p">,</span> <span class="n">out_tensors</span><span class="p">,</span> <span class="n">modPrefix</span><span class="p">,</span> <span class="n">api</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">api</span> <span class="o">==</span> <span class="s2">&quot;c&quot;</span><span class="p">:</span>
            <span class="n">retStr</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">// Define data for input and output tensors</span>
<span class="s2">&quot;&quot;&quot;</span>

            <span class="k">def</span> <span class="nf">writeTensorsHelper</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
                <span class="n">lenTensors</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>
                <span class="n">direction</span> <span class="o">=</span> <span class="s2">&quot;out&quot;</span> <span class="k">if</span> <span class="n">out</span> <span class="k">else</span> <span class="s2">&quot;in&quot;</span>
                <span class="n">ret</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
                <span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">direction</span><span class="si">}</span><span class="s2">put</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">_data&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">lenTensors</span><span class="p">)]</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tensors</span><span class="p">):</span>
                    <span class="n">ret</span> <span class="o">+=</span> <span class="s2">&quot;char &quot;</span> <span class="o">+</span> <span class="n">names</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;[&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">size</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;];</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="n">ret</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;void* </span><span class="si">{</span><span class="n">direction</span><span class="si">}</span><span class="s2">puts[] = </span><span class="se">{{</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">names</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;};</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="n">ret</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;struct </span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">direction</span><span class="si">}</span><span class="s2">puts </span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">direction</span><span class="si">}</span><span class="s2">puts = </span><span class="se">{{</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tensors</span><span class="p">):</span>
                    <span class="n">tensor_name</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;;&quot;</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span><span class="p">)</span>
                    <span class="n">ret</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;    .</span><span class="si">{</span><span class="n">tensor_name</span><span class="si">}</span><span class="s2"> = </span><span class="si">{</span><span class="n">names</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">,&quot;</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="n">ret</span> <span class="o">+=</span> <span class="s2">&quot;};</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="k">return</span> <span class="n">ret</span>

            <span class="n">retStr</span> <span class="o">+=</span> <span class="n">writeTensorsHelper</span><span class="p">(</span><span class="n">in_tensors</span><span class="p">,</span> <span class="n">modPrefix</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="n">retStr</span> <span class="o">+=</span> <span class="n">writeTensorsHelper</span><span class="p">(</span><span class="n">out_tensors</span><span class="p">,</span> <span class="n">modPrefix</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">retStr</span>
        <span class="k">elif</span> <span class="n">api</span> <span class="o">==</span> <span class="s2">&quot;packed&quot;</span><span class="p">:</span>
            <span class="n">retStr</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">// Define data for input and output tensors</span>
<span class="s2">&quot;&quot;&quot;</span>

            <span class="k">def</span> <span class="nf">writeTensorsHelper</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
                <span class="n">lenTensors</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>
                <span class="n">direction</span> <span class="o">=</span> <span class="s2">&quot;out&quot;</span> <span class="k">if</span> <span class="n">out</span> <span class="k">else</span> <span class="s2">&quot;in&quot;</span>
                <span class="n">ret</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
                <span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">direction</span><span class="si">}</span><span class="s2">put</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">_data&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">lenTensors</span><span class="p">)]</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tensors</span><span class="p">):</span>
                    <span class="n">ret</span> <span class="o">+=</span> <span class="s2">&quot;char &quot;</span> <span class="o">+</span> <span class="n">names</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;[&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">size</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;];</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="n">ret</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;void* </span><span class="si">{</span><span class="n">direction</span><span class="si">}</span><span class="s2">puts[] = </span><span class="se">{{</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">names</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;};</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="k">return</span> <span class="n">ret</span>

            <span class="n">retStr</span> <span class="o">+=</span> <span class="n">writeTensorsHelper</span><span class="p">(</span><span class="n">in_tensors</span><span class="p">,</span> <span class="n">modPrefix</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="n">retStr</span> <span class="o">+=</span> <span class="n">writeTensorsHelper</span><span class="p">(</span><span class="n">out_tensors</span><span class="p">,</span> <span class="n">modPrefix</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">retStr</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;api has to be either &#39;c&#39; or &#39;packed&#39;&quot;</span><span class="p">)</span>

    <span class="n">out</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="n">out</span> <span class="o">+=</span> <span class="n">generate_header</span><span class="p">()</span>
    <span class="n">includes</span> <span class="o">=</span> <span class="n">generate_aot_includes</span><span class="p">(</span><span class="n">workspace_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">api</span> <span class="o">==</span> <span class="s2">&quot;c&quot;</span><span class="p">:</span>
        <span class="n">includes</span> <span class="o">+=</span> <span class="s1">&#39;#include &quot;$</span><span class="si">{modPrefix}</span><span class="s1">.h&quot;</span><span class="se">\n</span><span class="s1">&#39;</span>

    <span class="n">out</span> <span class="o">+=</span> <span class="n">fill</span><span class="p">(</span><span class="n">includes</span><span class="p">,</span> <span class="n">modPrefix</span><span class="o">=</span><span class="n">modPrefix</span><span class="p">)</span>

    <span class="n">out</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>

    <span class="n">out</span> <span class="o">+=</span> <span class="n">writeTensors</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">in_tensors</span><span class="p">,</span> <span class="n">model_info</span><span class="o">.</span><span class="n">out_tensors</span><span class="p">,</span> <span class="n">modPrefix</span><span class="p">,</span> <span class="n">api</span><span class="p">)</span>

    <span class="n">logging_code</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">void TVMLogf(const char* msg, ...)</span>
<span class="s2">{</span>
<span class="s2">    va_list args;</span>
<span class="s2">    va_start(args, msg);</span>
<span class="s2">    DBGPRINTF(msg, args);</span>
<span class="s2">    va_end(args);</span>
<span class="s2">}</span>
<span class="s2">&quot;&quot;&quot;</span>

    <span class="n">out</span> <span class="o">+=</span> <span class="n">logging_code</span>

    <span class="k">if</span> <span class="n">workspace_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">workspace_code</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">#define WORKSPACE_SIZE ($</span><span class="si">{workspaceBytes}</span><span class="s2">)</span>
<span class="s2">static uint8_t g_aot_memory[WORKSPACE_SIZE];</span>
<span class="s2">tvm_workspace_t app_workspace;</span>
<span class="s2">&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">debug_arena</span><span class="p">:</span>  <span class="c1"># This will enable the feature only if it is not overwritten by the user/compiler</span>
            <span class="n">workspace_code</span> <span class="o">+=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">#ifndef DEBUG_ARENA_USAGE</span>
<span class="s2">#define DEBUG_ARENA_USAGE 1</span>
<span class="s2">#endif</span>

<span class="s2">#ifndef TVMAOT_DEBUG_ALLOCATIONS</span>
<span class="s2">#define TVMAOT_DEBUG_ALLOCATIONS 1</span>
<span class="s2">#endif</span>
<span class="s2">&quot;&quot;&quot;</span>

        <span class="n">workspace_code</span> <span class="o">+=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">#ifdef DEBUG_ARENA_USAGE</span>
<span class="s2">size_t max_arena_usage = 0;</span>
<span class="s2">#endif</span>

<span class="s2">tvm_crt_error_t TVMPlatformMemoryAllocate(size_t num_bytes, DLDevice dev, void** out_ptr)</span>
<span class="s2">{</span>
<span class="s2">#ifdef TVMAOT_DEBUG_ALLOCATIONS</span>
<span class="s2">    if (num_bytes &gt; (app_workspace.workspace + app_workspace.workspace_size - app_workspace.next_alloc))</span>
<span class="s2">    {</span>
<span class="s2">      TVMLogf(&quot;TVMPlatformMemoryAllocate(</span><span class="si">%lu</span><span class="s2">): Allocation would overflow arena!</span><span class="se">\\</span><span class="s2">n&quot;, num_bytes);</span>
<span class="s2">      return kTvmErrorPlatformNoMemory;</span>
<span class="s2">    }</span>
<span class="s2">#endif</span>
<span class="s2">    tvm_crt_error_t ret = StackMemoryManager_Allocate(&amp;app_workspace, num_bytes, out_ptr);</span>
<span class="s2">#ifdef DEBUG_ARENA_USAGE</span>
<span class="s2">  // Use this to estimate the required number of bytes for the arena</span>
<span class="s2">  size_t end = app_workspace.next_alloc-app_workspace.workspace;</span>
<span class="s2">  if (end &gt; max_arena_usage)</span>
<span class="s2">  {</span>
<span class="s2">    max_arena_usage = end;</span>
<span class="s2">  }</span>
<span class="s2">#endif</span>
<span class="s2">    return ret;</span>
<span class="s2">}</span>
<span class="s2">tvm_crt_error_t TVMPlatformMemoryFree(void* ptr, DLDevice dev)</span>
<span class="s2">{</span>
<span class="s2">#ifdef TVMAOT_DEBUG_ALLOCATIONS</span>
<span class="s2">    if ((uint8_t*)ptr &lt; app_workspace.workspace || (uint8_t*)ptr &gt;= app_workspace.next_alloc)</span>
<span class="s2">    {</span>
<span class="s2">      TVMLogf(&quot;TVMPlatformMemoryFree(%p): Invalid Memory region to be free&#39;d!</span><span class="se">\\</span><span class="s2">n&quot;, ptr);</span>
<span class="s2">      return kTvmErrorPlatformNoMemory;</span>
<span class="s2">    }</span>
<span class="s2">#endif</span>
<span class="s2">    return StackMemoryManager_Free(&amp;app_workspace, ptr);</span>
<span class="s2">}</span>
<span class="s2">&quot;&quot;&quot;</span>
        <span class="n">out</span> <span class="o">+=</span> <span class="n">fill</span><span class="p">(</span><span class="n">workspace_code</span><span class="p">,</span> <span class="n">workspaceBytes</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">workspace_size</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">workspace_code</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">tvm_crt_error_t TVMPlatformMemoryAllocate(size_t num_bytes, DLDevice dev, void** out_ptr)</span>
<span class="s2">{</span>
<span class="s2">    return kTvmErrorFunctionCallNotImplemented;</span>
<span class="s2">}</span>
<span class="s2">tvm_crt_error_t TVMPlatformMemoryFree(void* ptr, DLDevice dev)</span>
<span class="s2">{</span>
<span class="s2">    return kTvmErrorFunctionCallNotImplemented;</span>
<span class="s2">}</span>
<span class="s2">&quot;&quot;&quot;</span>
        <span class="n">out</span> <span class="o">+=</span> <span class="n">workspace_code</span>

    <span class="n">mainCode</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="k">if</span> <span class="n">api</span> <span class="o">==</span> <span class="s2">&quot;packed&quot;</span><span class="p">:</span>
        <span class="n">mainCode</span> <span class="o">+=</span> <span class="p">(</span>
            <span class="s2">&quot;int32_t $</span><span class="si">{modPrefix}</span><span class="s2">_run(void* args, void* type_code, int num_args,&quot;</span>
            <span class="o">+</span> <span class="s2">&quot; void* out_value, void* out_type_code, void* resource_handle);</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="n">mainCode</span> <span class="o">+=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">void __attribute__((noreturn)) TVMPlatformAbort(tvm_crt_error_t code)</span>
<span class="s2">{</span>
<span class="s2">    mlonmcu_exit(1);</span>
<span class="s2">}</span>

<span class="s2">TVM_DLL int TVMFuncRegisterGlobal(const char* name, TVMFunctionHandle f, int override)</span>
<span class="s2">{</span>
<span class="s2">    return 0;</span>
<span class="s2">}</span>

<span class="s2">int TVMWrap_Init()</span>
<span class="s2">{</span>
<span class="s2">&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">workspace_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">mainCode</span> <span class="o">+=</span> <span class="s2">&quot;    StackMemoryManager_Init(&amp;app_workspace, g_aot_memory, WORKSPACE_SIZE);&quot;</span>
    <span class="n">mainCode</span> <span class="o">+=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    return 0;  // TODO</span>
<span class="s2">}</span>

<span class="s2">void *TVMWrap_GetInputPtr(int index)</span>
<span class="s2">{</span>
<span class="s2">    return inputs[index];</span>
<span class="s2">}</span>

<span class="s2">size_t TVMWrap_GetInputSize(int index)</span>
<span class="s2">{</span>
<span class="s2">    $</span><span class="si">{inSizes}</span>

<span class="s2">    return sizes[index];</span>
<span class="s2">}</span>

<span class="s2">size_t TVMWrap_GetNumInputs()</span>
<span class="s2">{</span>
<span class="s2">    return $</span><span class="si">{numInputs}</span><span class="s2">;</span>
<span class="s2">}</span>

<span class="s2">int TVMWrap_Run()</span>
<span class="s2">{&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">api</span> <span class="o">==</span> <span class="s2">&quot;c&quot;</span><span class="p">:</span>
        <span class="n">mainCode</span> <span class="o">+=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    int ret_val = $</span><span class="si">{modPrefix}</span><span class="s2">_run(&amp;$</span><span class="si">{modPrefix}</span><span class="s2">_inputs, &amp;$</span><span class="si">{modPrefix}</span><span class="s2">_outputs);</span>
<span class="s2">    if (ret_val)</span>
<span class="s2">    {</span>
<span class="s2">        TVMPlatformAbort(kTvmErrorPlatformCheckFailure);</span>
<span class="s2">    }</span>
<span class="s2">&quot;&quot;&quot;</span>
    <span class="k">elif</span> <span class="n">api</span> <span class="o">==</span> <span class="s2">&quot;packed&quot;</span><span class="p">:</span>
        <span class="n">mainCode</span> <span class="o">+=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    static DLDevice fake_device = {kDLCPU, 0};</span>
<span class="s2">    static int64_t fake_dims = 0;</span>
<span class="s2">    static int64_t fake_shape = </span><span class="si">{0}</span><span class="s2">;</span>

<span class="s2">    DLTensor tensors[$</span><span class="si">{numInputs}</span><span class="s2"> + $</span><span class="si">{numOutputs}</span><span class="s2">];</span>
<span class="s2">    TVMValue values[$</span><span class="si">{numInputs}</span><span class="s2"> + $</span><span class="si">{numOutputs}</span><span class="s2">];</span>
<span class="s2">    int32_t typeids[$</span><span class="si">{numInputs}</span><span class="s2"> + $</span><span class="si">{numOutputs}</span><span class="s2">];</span>

<span class="s2">    for (size_t i = 0; i &lt; $</span><span class="si">{numInputs}</span><span class="s2">+$</span><span class="si">{numOutputs}</span><span class="s2">; i++)</span>
<span class="s2">    {</span>
<span class="s2">        tensors[i].device = fake_device;</span>
<span class="s2">        tensors[i].data = (i &lt; $</span><span class="si">{numInputs}</span><span class="s2">) ? inputs[i] : outputs[i - $</span><span class="si">{numInputs}</span><span class="s2">];</span>
<span class="s2">        tensors[i].shape = &amp;fake_shape;</span>
<span class="s2">        tensors[i].ndim = fake_dims;</span>
<span class="s2">        tensors[i].byte_offset = 0;</span>
<span class="s2">        tensors[i].strides = NULL;</span>
<span class="s2">        values[i].v_handle = &amp;tensors[i];</span>
<span class="s2">    }</span>

<span class="s2">    int ret_val = $</span><span class="si">{modPrefix}</span><span class="s2">_run(values, typeids, 0, NULL, 0, NULL);</span>
<span class="s2">    if (ret_val)</span>
<span class="s2">    {</span>
<span class="s2">        TVMPlatformAbort(kTvmErrorPlatformCheckFailure);</span>
<span class="s2">    }</span>
<span class="s2">    return 0;</span>

<span class="s2">&quot;&quot;&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;api can only be &#39;c&#39; or &#39;packed&#39;&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">workspace_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">mainCode</span> <span class="o">+=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">#if DEBUG_ARENA_USAGE</span>
<span class="s2">    DBGPRINTF(&quot;</span><span class="se">\\</span><span class="s2">nAoT executor arena max usage after model invocation: </span><span class="si">%lu</span><span class="s2"> bytes</span><span class="se">\\</span><span class="s2">n&quot;, max_arena_usage);</span>
<span class="s2">#endif  // DEBUG_ARENA_USAGE</span>
<span class="s2">&quot;&quot;&quot;</span>
    <span class="n">mainCode</span> <span class="o">+=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    return 0;  // TODO</span>
<span class="s2">}</span>

<span class="s2">void *TVMWrap_GetOutputPtr(int index)</span>
<span class="s2">{</span>
<span class="s2">    return outputs[index];</span>
<span class="s2">}</span>

<span class="s2">size_t TVMWrap_GetOutputSize(int index)</span>
<span class="s2">{</span>
<span class="s2">    $</span><span class="si">{outSizes}</span>

<span class="s2">    return sizes[index];</span>
<span class="s2">}</span>

<span class="s2">size_t TVMWrap_GetNumOutputs()</span>
<span class="s2">{</span>
<span class="s2">    return $</span><span class="si">{numOutputs}</span><span class="s2">;</span>
<span class="s2">}</span>
<span class="s2">&quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">+=</span> <span class="n">fill</span><span class="p">(</span>
        <span class="n">mainCode</span><span class="p">,</span>
        <span class="n">inSizes</span><span class="o">=</span><span class="n">getSizes</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">in_tensors</span><span class="p">),</span>
        <span class="n">outSizes</span><span class="o">=</span><span class="n">getSizes</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">out_tensors</span><span class="p">),</span>
        <span class="n">numInputs</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">in_tensors</span><span class="p">),</span>
        <span class="n">numOutputs</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">out_tensors</span><span class="p">),</span>
        <span class="n">modPrefix</span><span class="o">=</span><span class="n">modPrefix</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>



<div class="viewcode-block" id="write_tvmaot_wrapper">
<a class="viewcode-back" href="../../../../../mlonmcu.flow.tvm.backend.html#mlonmcu.flow.tvm.backend.wrapper.write_tvmaot_wrapper">[docs]</a>
<span class="k">def</span> <span class="nf">write_tvmaot_wrapper</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">model_info</span><span class="p">,</span> <span class="n">workspace_size</span><span class="p">,</span> <span class="n">mod_name</span><span class="p">,</span> <span class="n">api</span><span class="o">=</span><span class="s2">&quot;c&quot;</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">generate_tvmaot_wrapper</span><span class="p">(</span><span class="n">model_info</span><span class="p">,</span> <span class="n">workspace_size</span><span class="p">,</span> <span class="n">mod_name</span><span class="p">,</span> <span class="n">api</span><span class="o">=</span><span class="n">api</span><span class="p">)</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">text</span><span class="p">)</span></div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, TUM Department of Electrical and Computer Engineering - Chair of Electronic Design Automation.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>